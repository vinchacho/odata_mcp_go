# v1.6.5 HTTP Timeouts & SSE Improvements Design

**Status**: Implemented
**Author**: Claude + Vincent
**Date**: 2025-12-17
**Target Version**: v1.6.5

---

## Overview

v1.6.5 adds configurable HTTP timeouts and SSE observability improvements, addressing issues with large SAP metadata and dropped messages.

---

## Feature 1: HTTP Timeout Configuration

### Problem Statement

Large SAP OData services can take 60+ seconds to return `$metadata`. The default HTTP timeout (30s) causes:
- Metadata fetch failures
- Startup timeouts
- User confusion about why the bridge won't start

Different timeout requirements:
- Metadata fetch: Long (60s+) - large schema, one-time operation
- Regular requests: Short (30s) - interactive queries

### Solution

Add separate timeout configuration for different operation types:

```bash
--http-timeout 30           # Default request timeout (seconds)
--metadata-timeout 60       # Metadata fetch timeout (seconds)
```

### Architecture

```
┌─────────────────────────────────────────────────────┐
│                 TIMEOUT CONFIGURATION                │
├─────────────────────────────────────────────────────┤
│                                                     │
│  Startup Phase:                                     │
│    GET $metadata                                    │
│    ├── Uses: metadata_timeout (default: 60s)       │
│    └── Reason: Large schemas need more time        │
│                                                     │
│  Runtime Phase:                                     │
│    GET/POST/PUT/DELETE operations                   │
│    ├── Uses: http_timeout (default: 30s)           │
│    └── Reason: Interactive queries should fail fast│
│                                                     │
└─────────────────────────────────────────────────────┘
```

### CLI Interface

```bash
# Explicit timeout configuration
odata-mcp --service https://... --http-timeout 30 --metadata-timeout 120

# Environment variables
ODATA_HTTP_TIMEOUT=30
ODATA_METADATA_TIMEOUT=120
```

### Files Changed

| File | Changes |
|------|---------|
| `internal/config/config.go` | Add `HTTPTimeout`, `MetadataTimeout` fields |
| `cmd/odata-mcp/main.go` | Add CLI flags, env bindings |
| `internal/client/client.go` | Apply timeouts to HTTP client |
| `internal/bridge/bridge.go` | Use metadata timeout for initial fetch |

### Implementation

```go
// Config fields
type Config struct {
    HTTPTimeout     int  // seconds, default 30
    MetadataTimeout int  // seconds, default 60
}

// Client initialization
func NewODataClient(cfg *Config) *ODataClient {
    return &ODataClient{
        httpClient: &http.Client{
            Timeout: time.Duration(cfg.HTTPTimeout) * time.Second,
        },
        metadataTimeout: time.Duration(cfg.MetadataTimeout) * time.Second,
    }
}

// Metadata fetch with custom timeout
func (c *ODataClient) FetchMetadata(ctx context.Context, url string) ([]byte, error) {
    ctx, cancel := context.WithTimeout(ctx, c.metadataTimeout)
    defer cancel()
    // ... fetch with context
}
```

### Acceptance Criteria

- [ ] `--http-timeout` controls regular request timeout
- [ ] `--metadata-timeout` controls metadata fetch timeout
- [ ] Environment variables work: `ODATA_HTTP_TIMEOUT`, `ODATA_METADATA_TIMEOUT`
- [ ] Defaults: 30s for HTTP, 60s for metadata
- [ ] Timeout errors are clear and actionable

---

## Feature 2: SSE Dropped Message Logging

### Problem Statement

When SSE clients can't keep up with message rate, messages are dropped silently. Users have no visibility into:
- Whether messages are being dropped
- How many messages were lost
- When drops occur

### Solution

Add verbose logging and atomic counter for dropped messages:

```go
type SSETransport struct {
    droppedMessages atomic.Uint64
}

func (t *SSETransport) Send(msg []byte) {
    select {
    case t.messageChan <- msg:
        // Sent successfully
    default:
        // Channel full, message dropped
        count := t.droppedMessages.Add(1)
        if t.verbose {
            log.Printf("[VERBOSE] SSE: Message dropped (total: %d)", count)
        }
    }
}
```

### Files Changed

| File | Changes |
|------|---------|
| `internal/transport/sse.go` | Add dropped counter, verbose logging |

### Log Output Example

```
[VERBOSE] SSE: Message dropped (total: 1)
[VERBOSE] SSE: Message dropped (total: 2)
[VERBOSE] SSE: Client buffer full, consider increasing buffer size
```

### Acceptance Criteria

- [ ] Dropped messages logged when `--verbose` enabled
- [ ] Counter tracks total dropped messages
- [ ] No logging overhead when verbose disabled
- [ ] Counter is thread-safe (atomic)

---

## Feature 3: Linting Configuration

### Problem Statement

No consistent code quality checks. Different developers use different linters with different settings.

### Solution

Add `.golangci.yml` with conservative, production-focused linters:

```yaml
linters:
  enable:
    - errcheck      # Check error returns
    - govet         # Go vet checks
    - staticcheck   # Static analysis
    - unused        # Dead code detection

  disable:
    - gocyclo       # Too noisy for now
    - goconst       # False positives
    - gocognit      # Subjective metrics
```

### Files Changed

| File | Changes |
|------|---------|
| `.golangci.yml` | NEW - Linter configuration |

### Acceptance Criteria

- [ ] `golangci-lint run` passes
- [ ] CI runs linting on every PR
- [ ] No false positives from enabled linters

---

## Fix: Retry Configuration Not Applied

### Problem Statement

The `--retry-*` CLI flags were defined in v1.6.0 but never wired to the HTTP client. Configuration was parsed but ignored.

### Root Cause

```go
// In main.go - flags defined
cmd.Flags().Int("retry-max-attempts", 3, "...")

// In bridge.go - config read
retryMax := viper.GetInt("retry-max-attempts")

// MISSING: Pass to client
client := NewODataClient(...)  // Retry config not passed!
```

### Solution

Wire retry configuration during bridge initialization:

```go
// In bridge.go
client := NewODataClient(&ClientConfig{
    RetryMaxAttempts:     cfg.RetryMaxAttempts,
    RetryInitialBackoff:  cfg.RetryInitialBackoffMs,
    RetryMaxBackoff:      cfg.RetryMaxBackoffMs,
    RetryBackoffMultiplier: cfg.RetryBackoffMultiplier,
})
```

### Files Changed

| File | Changes |
|------|---------|
| `internal/bridge/bridge.go` | Pass retry config to client |
| `internal/client/client.go` | Accept and use retry config |

### Acceptance Criteria

- [ ] `--retry-max-attempts` actually affects retry behavior
- [ ] All retry flags work as documented
- [ ] Integration test confirms retry behavior

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Timeout too short causes failures | Medium | Medium | Conservative defaults, clear docs |
| Logging impacts performance | Low | Low | Only when verbose enabled |
| Linter too strict | Medium | Low | Start conservative, expand later |

---

## Testing Strategy

```bash
# Test timeout configuration
go test -run TestHTTPTimeout ./internal/client/
go test -run TestMetadataTimeout ./internal/client/

# Test SSE dropping
go test -run TestSSEDroppedMessages ./internal/transport/

# Test retry wiring
go test -run TestRetryConfig ./internal/bridge/

# Run linter
golangci-lint run ./...
```

---

## Success Metrics

| Metric | Target |
|--------|--------|
| Large metadata services start successfully | 100% |
| Dropped messages visible in verbose mode | Yes |
| `golangci-lint run` passes | 0 errors |
| Retry configuration works | Verified in tests |
